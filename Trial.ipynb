{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    # 构造函数\n",
    "    def __init__(self, data_tensor, target_tensor):\n",
    "        self.data_tensor = data_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "    # 返回数据集大小\n",
    "    def __len__(self):\n",
    "        return self.data_tensor.size(0)\n",
    "    # 返回索引的数据与标签\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_tensor[index], self.target_tensor[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "file6 = \"D:\\\\320 Deep-learning\\\\1111_S6_time1000.mat\"\n",
    "data6 = loadmat(file6)\n",
    "file12 = \"D:\\\\320 Deep-learning\\\\1111_S12_time1000.mat\"\n",
    "data12 = loadmat(file12)\n",
    "file24 = \"D:\\\\320 Deep-learning\\\\1111_S24_time1000.mat\"\n",
    "data24 = loadmat(file24)\n",
    "\"\"\" experiment\n",
    "X_train = data['Input']\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = data['Stability']\n",
    "y_train = y_train.astype(np.float32) / 255\n",
    "y_train = torch.tensor(y_train)\n",
    "len_train = len(y_train)\n",
    "X_test = data['PreInput']\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = data['PreSta']\n",
    "y_test = y_test.astype(np.float32) / 255\n",
    "y_test = torch.tensor(y_test)\n",
    "len_test = len(y_test) \"\"\"\n",
    "\n",
    "#simulation\n",
    "X6 = data6['Input_Abundance']\n",
    "X6 = torch.from_numpy(X6).float()\n",
    "y6 = data6['Std_total']\n",
    "y6 = -np.log10(y6)\n",
    "y6 = y6.T\n",
    "y6 = torch.from_numpy(y6).float()\n",
    "\n",
    "X12 = data12['Input_RelativeAbundance']\n",
    "X12 = torch.from_numpy(X12).float()\n",
    "y12 = data12['Stability']\n",
    "y12 = torch.from_numpy(y12).float()\n",
    "\n",
    "X24 = data24['Input_RelativeAbundance']\n",
    "X24 = torch.from_numpy(X24).float()\n",
    "y24= data24['Stability']\n",
    "y24 = torch.from_numpy(y24).float()\n",
    "\n",
    "t = 500\n",
    "X_train = X6[0:t,:]\n",
    "y_train = y6[0:t]\n",
    "\n",
    "X_test = X6[t:1000,:]\n",
    "y_test = y6[t:1000]\n",
    "\n",
    "len_train = len(y_train)\n",
    "len_test = len(y_test)\n",
    "\n",
    "# 将数据封装成Dataset\n",
    "train_dataset = MyDataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 50\n",
    "\n",
    "\"\"\" 需不需要随机采样？ 随机采样和shuffle是不是一样的 \"\"\"\n",
    "\"\"\" # define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, \n",
    "                                           sampler = train_sampler, num_workers = num_workers) \"\"\"\n",
    "\n",
    "# prepare data loaders\n",
    "train_dataloader = DataLoader(dataset = train_dataset, # 传入的数据集, 必须参数\n",
    "                               batch_size = batch_size,       # 输出的batch大小\n",
    "                               shuffle = True,       # 数据是否打乱\n",
    "                               num_workers = num_workers)      # 进程数, 0表示只有主进程\n",
    "\n",
    "#for data,target in train_dataloader:\n",
    "#    print(target.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(54, 1)\n",
    "        self.fc2 = nn.Linear(54, 54)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 7.373371\n",
      "Train loss decreased (inf --> 7.373371).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 5.200790\n",
      "Train loss decreased (7.373371 --> 5.200790).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 4.534297\n",
      "Train loss decreased (5.200790 --> 4.534297).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 4.361402\n",
      "Train loss decreased (4.534297 --> 4.361402).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 4.250099\n",
      "Train loss decreased (4.361402 --> 4.250099).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 4.146095\n",
      "Train loss decreased (4.250099 --> 4.146095).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 4.046939\n",
      "Train loss decreased (4.146095 --> 4.046939).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 3.982656\n",
      "Train loss decreased (4.046939 --> 3.982656).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 3.929502\n",
      "Train loss decreased (3.982656 --> 3.929502).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 3.889799\n",
      "Train loss decreased (3.929502 --> 3.889799).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 3.857612\n",
      "Train loss decreased (3.889799 --> 3.857612).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 3.841951\n",
      "Train loss decreased (3.857612 --> 3.841951).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 3.811897\n",
      "Train loss decreased (3.841951 --> 3.811897).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 3.798825\n",
      "Train loss decreased (3.811897 --> 3.798825).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 3.787541\n",
      "Train loss decreased (3.798825 --> 3.787541).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 3.783069\n",
      "Train loss decreased (3.787541 --> 3.783069).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 3.774538\n",
      "Train loss decreased (3.783069 --> 3.774538).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 3.768552\n",
      "Train loss decreased (3.774538 --> 3.768552).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 3.757221\n",
      "Train loss decreased (3.768552 --> 3.757221).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 3.754014\n",
      "Train loss decreased (3.757221 --> 3.754014).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 3.751423\n",
      "Train loss decreased (3.754014 --> 3.751423).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 3.746413\n",
      "Train loss decreased (3.751423 --> 3.746413).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 3.746755\n",
      "Epoch: 24 \tTraining Loss: 3.740183\n",
      "Train loss decreased (3.746413 --> 3.740183).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 3.738621\n",
      "Train loss decreased (3.740183 --> 3.738621).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 3.732175\n",
      "Train loss decreased (3.738621 --> 3.732175).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 3.730280\n",
      "Train loss decreased (3.732175 --> 3.730280).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 3.724203\n",
      "Train loss decreased (3.730280 --> 3.724203).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 3.722918\n",
      "Train loss decreased (3.724203 --> 3.722918).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 3.717908\n",
      "Train loss decreased (3.722918 --> 3.717908).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 3.716324\n",
      "Train loss decreased (3.717908 --> 3.716324).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 3.714901\n",
      "Train loss decreased (3.716324 --> 3.714901).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 3.715898\n",
      "Epoch: 34 \tTraining Loss: 3.711877\n",
      "Train loss decreased (3.714901 --> 3.711877).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 3.710775\n",
      "Train loss decreased (3.711877 --> 3.710775).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 3.705397\n",
      "Train loss decreased (3.710775 --> 3.705397).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 3.701684\n",
      "Train loss decreased (3.705397 --> 3.701684).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 3.694790\n",
      "Train loss decreased (3.701684 --> 3.694790).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 3.696668\n",
      "Epoch: 40 \tTraining Loss: 3.703263\n",
      "Epoch: 41 \tTraining Loss: 3.690501\n",
      "Train loss decreased (3.694790 --> 3.690501).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 3.694212\n",
      "Epoch: 43 \tTraining Loss: 3.685949\n",
      "Train loss decreased (3.690501 --> 3.685949).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 3.682537\n",
      "Train loss decreased (3.685949 --> 3.682537).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 3.690041\n",
      "Epoch: 46 \tTraining Loss: 3.675597\n",
      "Train loss decreased (3.682537 --> 3.675597).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 3.678813\n",
      "Epoch: 48 \tTraining Loss: 3.679946\n",
      "Epoch: 49 \tTraining Loss: 3.671918\n",
      "Train loss decreased (3.675597 --> 3.671918).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 3.673156\n",
      "Epoch: 51 \tTraining Loss: 3.669451\n",
      "Train loss decreased (3.671918 --> 3.669451).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 3.664523\n",
      "Train loss decreased (3.669451 --> 3.664523).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 3.664916\n",
      "Epoch: 54 \tTraining Loss: 3.664161\n",
      "Train loss decreased (3.664523 --> 3.664161).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 3.662825\n",
      "Train loss decreased (3.664161 --> 3.662825).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 3.664713\n",
      "Epoch: 57 \tTraining Loss: 3.659405\n",
      "Train loss decreased (3.662825 --> 3.659405).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 3.659067\n",
      "Train loss decreased (3.659405 --> 3.659067).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 3.655483\n",
      "Train loss decreased (3.659067 --> 3.655483).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 3.648672\n",
      "Train loss decreased (3.655483 --> 3.648672).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 3.651833\n",
      "Epoch: 62 \tTraining Loss: 3.649017\n",
      "Epoch: 63 \tTraining Loss: 3.652800\n",
      "Epoch: 64 \tTraining Loss: 3.648186\n",
      "Train loss decreased (3.648672 --> 3.648186).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 3.645944\n",
      "Train loss decreased (3.648186 --> 3.645944).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 3.645051\n",
      "Train loss decreased (3.645944 --> 3.645051).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 3.643813\n",
      "Train loss decreased (3.645051 --> 3.643813).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 3.641640\n",
      "Train loss decreased (3.643813 --> 3.641640).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 3.644481\n",
      "Epoch: 70 \tTraining Loss: 3.641585\n",
      "Train loss decreased (3.641640 --> 3.641585).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 3.639203\n",
      "Train loss decreased (3.641585 --> 3.639203).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 3.635400\n",
      "Train loss decreased (3.639203 --> 3.635400).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 3.639842\n",
      "Epoch: 74 \tTraining Loss: 3.633354\n",
      "Train loss decreased (3.635400 --> 3.633354).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 3.630076\n",
      "Train loss decreased (3.633354 --> 3.630076).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 3.637296\n",
      "Epoch: 77 \tTraining Loss: 3.631190\n",
      "Epoch: 78 \tTraining Loss: 3.631615\n",
      "Epoch: 79 \tTraining Loss: 3.627944\n",
      "Train loss decreased (3.630076 --> 3.627944).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 3.630031\n",
      "Epoch: 81 \tTraining Loss: 3.628709\n",
      "Epoch: 82 \tTraining Loss: 3.626913\n",
      "Train loss decreased (3.627944 --> 3.626913).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 3.623919\n",
      "Train loss decreased (3.626913 --> 3.623919).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 3.630667\n",
      "Epoch: 85 \tTraining Loss: 3.622521\n",
      "Train loss decreased (3.623919 --> 3.622521).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 3.625722\n",
      "Epoch: 87 \tTraining Loss: 3.623057\n",
      "Epoch: 88 \tTraining Loss: 3.640634\n",
      "Epoch: 89 \tTraining Loss: 3.634321\n",
      "Epoch: 90 \tTraining Loss: 3.630032\n",
      "Epoch: 91 \tTraining Loss: 3.620871\n",
      "Train loss decreased (3.622521 --> 3.620871).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 3.625689\n",
      "Epoch: 93 \tTraining Loss: 3.619544\n",
      "Train loss decreased (3.620871 --> 3.619544).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 3.621167\n",
      "Epoch: 95 \tTraining Loss: 3.623907\n",
      "Epoch: 96 \tTraining Loss: 3.618913\n",
      "Train loss decreased (3.619544 --> 3.618913).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 3.615817\n",
      "Train loss decreased (3.618913 --> 3.615817).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 3.616480\n",
      "Epoch: 99 \tTraining Loss: 3.613474\n",
      "Train loss decreased (3.615817 --> 3.613474).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 3.613921\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "train_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    # train the model #\n",
    "    loss = 0\n",
    "    for data, target in train_dataloader: \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len_train\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    \n",
    "    # save model\n",
    "    if train_loss <= train_loss_min:\n",
    "        print('Train loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        train_loss_min,\n",
    "        train_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        train_loss_min = train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 2.3721,  0.6350,  2.0054,  2.9148,  0.1143,  1.1478, -2.7562,  1.1001,\n",
      "          3.0882, -2.6150,  2.9371,  2.6371, -1.0636, -0.0946, -0.5080, -0.8238,\n",
      "         -0.6493,  3.9894,  2.6490, -1.5694,  0.9870,  1.7820,  0.9278,  0.0675,\n",
      "         -2.9977, -5.3900, -1.6041,  4.2010, -2.2948, -1.9697, -6.8755,  0.0745,\n",
      "          0.5881,  0.9349, -3.2190,  1.1640, -0.7082,  2.0214, -1.2555,  0.8160,\n",
      "          1.0351,  3.1720, -1.3668, -0.9024,  2.7254, -1.4308,  1.3055,  0.5564,\n",
      "         -5.4227, -4.2962, -0.6810,  2.5872,  3.7984,  2.5706]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "#for name, parameters in model.named_parameters():\n",
    "#    print(name, ':', parameters.size())\n",
    "print(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.000474\n",
      "\n",
      "\n",
      "Test Accuracy: 84% (424/500)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "OUTPUT = []\n",
    "S_pre = torch.Tensor(len_test,1)\n",
    "S_true = torch.Tensor(len_test,1)\n",
    "\n",
    "for i in range(len_test):\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(X_test[i])\n",
    "    o = output.tolist()\n",
    "    OUTPUT.append(o)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, y_test[i])\n",
    "    # update test loss \n",
    "    test_loss += loss.item()\n",
    "    # use the output to predict y\n",
    "    if(output>2): S_pre[i] = 0\n",
    "    else: S_pre[i] = 1\n",
    "    if(y_test[i]>2): S_true[i] = 0\n",
    "    else: S_true[i] = 1   \n",
    "    correct += S_pre[i].eq(S_true[i])\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len_test\n",
    "correct_ratio = correct/len_test\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "    100*correct_ratio,correct,len_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
