{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建子类\n",
    "class subDataset(Data.Dataset):\n",
    "    #初始化，定义数据内容和标签\n",
    "    def __init__(self, Data, Label):\n",
    "        self.Data = Data\n",
    "        self.Label = Label\n",
    "    #返回数据集大小\n",
    "    def __len__(self):\n",
    "        return len(self.Data)\n",
    "    #得到数据内容和标签\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.Tensor(self.Data[index])\n",
    "        label = torch.Tensor(self.Label[index])\n",
    "        return data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    # 构造函数\n",
    "    def __init__(self, data_tensor, target_tensor):\n",
    "        self.data_tensor = data_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "    # 返回数据集大小\n",
    "    def __len__(self):\n",
    "        return self.data_tensor.size(0)\n",
    "    # 返回索引的数据与标签\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_tensor[index], self.target_tensor[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "#file = \"D:\\\\310 科研\\\\000 Simulation\\\\1110_time1000_S6.mat\"\n",
    "file6 = \"D:\\\\310 科研\\\\000 Simulation\\\\1111_S6_time1000.mat\"\n",
    "data6 = loadmat(file6)\n",
    "file12 = \"D:\\\\310 科研\\\\000 Simulation\\\\1111_S12_time1000.mat\"\n",
    "data12 = loadmat(file12)\n",
    "file24 = \"D:\\\\310 科研\\\\000 Simulation\\\\1111_S24_time1000.mat\"\n",
    "data24 = loadmat(file24)\n",
    "\"\"\" experiment\n",
    "X_train = data['Input']\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = data['Stability']\n",
    "y_train = y_train.astype(np.float32) / 255\n",
    "y_train = torch.tensor(y_train)\n",
    "len_train = len(y_train)\n",
    "X_test = data['PreInput']\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = data['PreSta']\n",
    "y_test = y_test.astype(np.float32) / 255\n",
    "y_test = torch.tensor(y_test)\n",
    "len_test = len(y_test) \"\"\"\n",
    "\n",
    "#simulation\n",
    "X6 = data6['Input_Abundance']\n",
    "X6 = torch.from_numpy(X6).float()\n",
    "y6 = data6['Std_total']\n",
    "y6 = y6.T\n",
    "y6 = torch.from_numpy(y6).float()\n",
    "\n",
    "X12 = data12['Input_RelativeAbundance']\n",
    "X12 = torch.from_numpy(X12).float()\n",
    "y12 = data12['Stability']\n",
    "y12 = torch.from_numpy(y12).float()\n",
    "\n",
    "X24 = data24['Input_RelativeAbundance']\n",
    "X24 = torch.from_numpy(X24).float()\n",
    "y24= data24['Stability']\n",
    "y24 = torch.from_numpy(y24).float()\n",
    "\n",
    "X_train = X6[0:300,:]\n",
    "y_train = y6[0:300]\n",
    "#y_train = X6[0:300,0]\n",
    "\n",
    "X_test = X6[300:1000,:]\n",
    "y_test = y6[300:1000]\n",
    "#y_test = X6[300:1000,0]\n",
    "\n",
    "len_train = len(y_train)\n",
    "len_test = len(y_test)\n",
    "\n",
    "# 生成数据\n",
    "train_data_tensor = X_train\n",
    "train_target_tensor = y_train\n",
    "test_data_tensor = X_test\n",
    "test_target_tensor = y_test\n",
    "\n",
    "# 将数据封装成Dataset\n",
    "train_dataset = MyDataset(train_data_tensor, train_target_tensor)\n",
    "test_dataset = MyDataset(test_data_tensor, test_target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 10\n",
    "\n",
    "\"\"\" 需不需要随机采样？ 随机采样和shuffle是不是一样的 \"\"\"\n",
    "\"\"\" # define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(valid_index)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, \n",
    "                                           sampler = train_sampler, num_workers = num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size,\n",
    "                                         num_workers = num_workers) \"\"\"\n",
    "\n",
    "# prepare data loaders\n",
    "train_dataloader = DataLoader(dataset = train_dataset, # 传入的数据集, 必须参数\n",
    "                               batch_size = batch_size,       # 输出的batch大小\n",
    "                               shuffle = True,       # 数据是否打乱\n",
    "                               num_workers = num_workers)      # 进程数, 0表示只有主进程\n",
    "test_dataloader = DataLoader(dataset = test_dataset, # 传入的数据集, 必须参数\n",
    "                               batch_size = batch_size,       # 输出的batch大小\n",
    "                               shuffle = True,       # 数据是否打乱\n",
    "                               num_workers = num_workers)      # 进程数, 0表示只有主进程\n",
    "#for data,target in train_dataloader:\n",
    "#    print(target.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=54, out_features=1, bias=True)\n",
      "  (fc2): Linear(in_features=54, out_features=54, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(54, 1)\n",
    "        self.fc2 = nn.Linear(54, 54)\n",
    "    def forward(self,x):\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.424133\n",
      "Train loss decreased (inf --> 0.424133).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.231207\n",
      "Train loss decreased (0.424133 --> 0.231207).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.169836\n",
      "Train loss decreased (0.231207 --> 0.169836).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.142397\n",
      "Train loss decreased (0.169836 --> 0.142397).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.127564\n",
      "Train loss decreased (0.142397 --> 0.127564).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.118540\n",
      "Train loss decreased (0.127564 --> 0.118540).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.112637"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss decreased (0.118540 --> 0.112637).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.108533\n",
      "Train loss decreased (0.112637 --> 0.108533).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.105598\n",
      "Train loss decreased (0.108533 --> 0.105598).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.103423\n",
      "Train loss decreased (0.105598 --> 0.103423).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.101753\n",
      "Train loss decreased (0.103423 --> 0.101753).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.100445\n",
      "Train loss decreased (0.101753 --> 0.100445).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.099404\n",
      "Train loss decreased (0.100445 --> 0.099404).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.098561\n",
      "Train loss decreased (0.099404 --> 0.098561).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.097881\n",
      "Train loss decreased (0.098561 --> 0.097881).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.097327\n",
      "Train loss decreased (0.097881 --> 0.097327).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.096840\n",
      "Train loss decreased (0.097327 --> 0.096840).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.096447\n",
      "Train loss decreased (0.096840 --> 0.096447).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.096086\n",
      "Train loss decreased (0.096447 --> 0.096086).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.095781\n",
      "Train loss decreased (0.096086 --> 0.095781).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.095527\n",
      "Train loss decreased (0.095781 --> 0.095527).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.095296\n",
      "Train loss decreased (0.095527 --> 0.095296).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.095079\n",
      "Train loss decreased (0.095296 --> 0.095079).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.094889\n",
      "Train loss decreased (0.095079 --> 0.094889).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.094696\n",
      "Train loss decreased (0.094889 --> 0.094696).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.094553\n",
      "Train loss decreased (0.094696 --> 0.094553).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.094400\n",
      "Train loss decreased (0.094553 --> 0.094400).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.094255\n",
      "Train loss decreased (0.094400 --> 0.094255).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.094129\n",
      "Train loss decreased (0.094255 --> 0.094129).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.093997\n",
      "Train loss decreased (0.094129 --> 0.093997).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.093884\n",
      "Train loss decreased (0.093997 --> 0.093884).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.093773\n",
      "Train loss decreased (0.093884 --> 0.093773).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.093674\n",
      "Train loss decreased (0.093773 --> 0.093674).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.093557\n",
      "Train loss decreased (0.093674 --> 0.093557).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.093468\n",
      "Train loss decreased (0.093557 --> 0.093468).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.093376\n",
      "Train loss decreased (0.093468 --> 0.093376).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.093270\n",
      "Train loss decreased (0.093376 --> 0.093270).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.093168\n",
      "Train loss decreased (0.093270 --> 0.093168).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.093086\n",
      "Train loss decreased (0.093168 --> 0.093086).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.092997\n",
      "Train loss decreased (0.093086 --> 0.092997).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.092926\n",
      "Train loss decreased (0.092997 --> 0.092926).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.092827\n",
      "Train loss decreased (0.092926 --> 0.092827).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.092739\n",
      "Train loss decreased (0.092827 --> 0.092739).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.092658\n",
      "Train loss decreased (0.092739 --> 0.092658).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.092574\n",
      "Train loss decreased (0.092658 --> 0.092574).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.092496\n",
      "Train loss decreased (0.092574 --> 0.092496).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.092415\n",
      "Train loss decreased (0.092496 --> 0.092415).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.092352\n",
      "Train loss decreased (0.092415 --> 0.092352).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.092261\n",
      "Train loss decreased (0.092352 --> 0.092261).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.092178\n",
      "Train loss decreased (0.092261 --> 0.092178).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 0.092104\n",
      "Train loss decreased (0.092178 --> 0.092104).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.092023\n",
      "Train loss decreased (0.092104 --> 0.092023).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.091941\n",
      "Train loss decreased (0.092023 --> 0.091941).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.091869\n",
      "Train loss decreased (0.091941 --> 0.091869).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.091804\n",
      "Train loss decreased (0.091869 --> 0.091804).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.091732\n",
      "Train loss decreased (0.091804 --> 0.091732).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.091644\n",
      "Train loss decreased (0.091732 --> 0.091644).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.091568\n",
      "Train loss decreased (0.091644 --> 0.091568).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.091500\n",
      "Train loss decreased (0.091568 --> 0.091500).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.091426\n",
      "Train loss decreased (0.091500 --> 0.091426).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.091352\n",
      "Train loss decreased (0.091426 --> 0.091352).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.091269\n",
      "Train loss decreased (0.091352 --> 0.091269).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.091199\n",
      "Train loss decreased (0.091269 --> 0.091199).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.091132\n",
      "Train loss decreased (0.091199 --> 0.091132).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.091060\n",
      "Train loss decreased (0.091132 --> 0.091060).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 0.090987\n",
      "Train loss decreased (0.091060 --> 0.090987).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.090918\n",
      "Train loss decreased (0.090987 --> 0.090918).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.090841\n",
      "Train loss decreased (0.090918 --> 0.090841).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.090773\n",
      "Train loss decreased (0.090841 --> 0.090773).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.090701\n",
      "Train loss decreased (0.090773 --> 0.090701).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.090640\n",
      "Train loss decreased (0.090701 --> 0.090640).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 0.090559\n",
      "Train loss decreased (0.090640 --> 0.090559).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.090496\n",
      "Train loss decreased (0.090559 --> 0.090496).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.090427\n",
      "Train loss decreased (0.090496 --> 0.090427).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 0.090364\n",
      "Train loss decreased (0.090427 --> 0.090364).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 0.090298\n",
      "Train loss decreased (0.090364 --> 0.090298).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.090215\n",
      "Train loss decreased (0.090298 --> 0.090215).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 0.090164\n",
      "Train loss decreased (0.090215 --> 0.090164).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.090080\n",
      "Train loss decreased (0.090164 --> 0.090080).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.090021\n",
      "Train loss decreased (0.090080 --> 0.090021).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.089952\n",
      "Train loss decreased (0.090021 --> 0.089952).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 0.089880\n",
      "Train loss decreased (0.089952 --> 0.089880).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.089823\n",
      "Train loss decreased (0.089880 --> 0.089823).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 0.089757\n",
      "Train loss decreased (0.089823 --> 0.089757).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.089680\n",
      "Train loss decreased (0.089757 --> 0.089680).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.089616\n",
      "Train loss decreased (0.089680 --> 0.089616).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 0.089550\n",
      "Train loss decreased (0.089616 --> 0.089550).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.089486\n",
      "Train loss decreased (0.089550 --> 0.089486).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 0.089418\n",
      "Train loss decreased (0.089486 --> 0.089418).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 0.089364\n",
      "Train loss decreased (0.089418 --> 0.089364).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.089303\n",
      "Train loss decreased (0.089364 --> 0.089303).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.089230\n",
      "Train loss decreased (0.089303 --> 0.089230).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 0.089168\n",
      "Train loss decreased (0.089230 --> 0.089168).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.089107\n",
      "Train loss decreased (0.089168 --> 0.089107).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 0.089044\n",
      "Train loss decreased (0.089107 --> 0.089044).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.088990\n",
      "Train loss decreased (0.089044 --> 0.088990).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 0.088918\n",
      "Train loss decreased (0.088990 --> 0.088918).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 0.088855\n",
      "Train loss decreased (0.088918 --> 0.088855).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 0.088791\n",
      "Train loss decreased (0.088855 --> 0.088791).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.088745\n",
      "Train loss decreased (0.088791 --> 0.088745).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "train_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    # train the model #\n",
    "    loss = 0\n",
    "    for data, target in train_dataloader: \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len_train\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    \n",
    "    # save model\n",
    "    if train_loss <= train_loss_min:\n",
    "        print('Train loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        train_loss_min,\n",
    "        train_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        train_loss_min = train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight : torch.Size([1, 54])\n",
      "fc1.bias : torch.Size([1])\n",
      "fc2.weight : torch.Size([54, 54])\n",
      "fc2.bias : torch.Size([54])\n",
      "Parameter containing:\n",
      "tensor([[-0.1086,  0.0007, -0.0713, -0.3525, -0.3356, -0.1838, -0.1133,  0.0976,\n",
      "         -0.2465, -0.1674, -0.5301,  0.0696, -0.2983, -0.3626, -0.3986,  0.0158,\n",
      "         -0.1299, -0.3705, -0.3369, -0.1123,  0.0098, -0.3401, -0.4739, -0.0810,\n",
      "          0.1148, -0.3076, -0.3868, -0.3000, -0.1762,  0.0462,  0.3125, -0.3655,\n",
      "         -0.4192, -0.4167, -0.1766, -0.2212,  0.0481, -0.1947,  0.1107,  0.1448,\n",
      "         -0.2697, -0.2183, -0.1916, -0.2666,  0.0340,  0.0985, -0.3640,  0.0767,\n",
      "          0.1735, -0.0436, -0.3005, -0.4003, -0.2734, -0.0422]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "for name, parameters in model.named_parameters():\n",
    "    print(name, ':', parameters.size())\n",
    "print(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.089802\n",
      "\n",
      "\n",
      "Test Accuracy: 12% (87/700)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "OUTPUT = []\n",
    "S_pre = torch.Tensor(len_test,1)\n",
    "S_true = torch.Tensor(len_test,1)\n",
    "\n",
    "for i in range(len_test):\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(X_test[i])\n",
    "    o = output.tolist()\n",
    "    OUTPUT.append(o)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, y_test[i])\n",
    "    # update test loss \n",
    "    test_loss += loss.item()\n",
    "    # use the output to predict y\n",
    "    if(output<0.01): S_pre[i] = 0\n",
    "    else: S_pre[i] = 1\n",
    "    if(y_test[i]<0.01): S_true[i] = 0\n",
    "    else: S_true[i] = 1   \n",
    "    correct += S_pre[i].eq(S_true[i])\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len_test\n",
    "correct_ratio = correct/len_test\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "    100*correct_ratio,correct,len_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
